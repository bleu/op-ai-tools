{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for exploring variations on LLM, embeddings, RAG arch and prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBs that are going to be retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = [\n",
    "    \"full_docs\",\n",
    "    \"fragments_docs\",\n",
    "    \"posts_forum\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options of embedding, chat and vectorscore to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to test the open ai api for embeddings\n",
    "embedding_models = [\"text-embedding-3-small\", \"text-embedding-3-large\", \"text-embedding-ada-002\"]\n",
    "\n",
    "# we are going to test the open ai api for chat\n",
    "chat_models = [\"gpt-3.5-turbo-0125\", \"gpt-3.5-turbo-instruct\", \"gpt-4o\", \"gpt-4o-2024-05-13\"]\n",
    "\n",
    "# we are going to use faiss\n",
    "vectorstores = ['faiss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, asyncio, time, re\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "\n",
    "# embedding and chat\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# openai api key\n",
    "openai_api_key = getpass(\"Enter the OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# vectorstore\n",
    "if 'faiss' in vectorstores:\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# for tracking\n",
    "import weave\n",
    "from weave import Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'How can I get involved in the Optimism Collective?',\n",
       "  'expected': 'You can get involved in the Optimism Collective by following three principles: do what you love, fix problems together, and do it with optimism. There are various ways to contribute, such as helping with translations, improving documentation, participating in local events, or joining support programs like the NERD program.'},\n",
       " {'question': 'Can Optimism currently censor user transactions?',\n",
       "  'expected': \"No, even though the Optimism Foundation currently runs the sole sequencer on OP Mainnet, it does not have the ability to censor user transactions. However, decentralizing the sequencer is still a goal to further enhance the network's robustness and inclusivity.\"},\n",
       " {'question': 'Who are the members of the proposed Decentralized Finance Governance Committee for Optimism?',\n",
       "  'expected': 'The committee consists of Katie Garcia, GFX Labs, Flipside Crypto, StableNode, and Linda Xie.'},\n",
       " {'question': 'What were some of the outcomes and feedback from Season 3 of the Grants Council?',\n",
       "  'expected': 'Season 3 saw the review of over 150 proposals, with feedback indicating a significant improvement in the grants process. The Council received an NPS of 9.2 out of 10 from successful proposers, highlighting its emphasis on transparency and efficiency.'},\n",
       " {'question': \"Why is there a 'no-sale' rule for growth experiments?\",\n",
       "  'expected': \"The 'no-sale' rule for growth experiments ensures that grants are used to drive consumer interactions and usage of applications on Optimism, rather than being sold by the protocol that receives them. This rule makes it explicit that the grants should be distributed to drive consumer usage in line with the experiment outlined in the grant.\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = \"../002_create_test_dataset/questions_test_dataset.csv\"\n",
    "test_dataset = pd.read_csv(test_path)\n",
    "\n",
    "# drop origin\n",
    "test_dataset = test_dataset.drop(columns=['origin'])\n",
    "\n",
    "# change columns\n",
    "test_dataset = test_dataset.rename(columns={'answer': 'expected'})\n",
    "\n",
    "# sample 50 questions\n",
    "test_dataset = test_dataset.sample(5, random_state=42)\n",
    "\n",
    "# as dict\n",
    "test_dataset = test_dataset.to_dict(orient='records')\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General definitions for accessing data and creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(dbs, model_embeddings, vectorstore = 'faiss'):\n",
    "    embeddings = OpenAIEmbeddings(model=model_embeddings, openai_api_key=openai_api_key)\n",
    "    if vectorstore == 'faiss':\n",
    "        dbs = [f\"dbs/{name}_db/faiss/{model_embeddings}\" for name in dbs]\n",
    "        dbs = [FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True) for db_path in dbs]\n",
    "        db = dbs[0]\n",
    "        for db_ in dbs[1:]:\n",
    "            db.merge_from(db_)\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGModel():\n",
    "    @weave.op()\n",
    "    def __init__(self, dbs, model_embeddings, chat_pars, prompt_template, vectorstore = 'faiss'):\n",
    "        self.dbs_name = dbs\n",
    "        self.embeddings_name = model_embeddings\n",
    "        self.vectorstore_name = vectorstore\n",
    "\n",
    "        self.db = load_db(dbs, model_embeddings, vectorstore)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        llm = ChatOpenAI(**chat_pars, openai_api_key=openai_api_key)\n",
    "        self.chain = prompt | llm\n",
    "\n",
    "        self.retriever = None\n",
    "            \n",
    "    @weave.op()\n",
    "    def def_faiss_retriever(self, **retriever_kwargs):\n",
    "        self.retriever = self.db.as_retriever(**retriever_kwargs)\n",
    "        return self.retriever\n",
    "    \n",
    "    @weave.op()\n",
    "    def find_similar_docs(self, query):\n",
    "        if self.vectorstore_name == 'faiss':\n",
    "            return self.retriever.invoke(query)\n",
    "        \n",
    "    @weave.op()\n",
    "    def get_answer(self, question: str):\n",
    "        context = self.find_similar_docs(question)\n",
    "\n",
    "        response = self.chain.invoke(\n",
    "            {\n",
    "                \"context\": context,\n",
    "                \"question\": question,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return {\"context\": str(context), \"answer\": response.content}\n",
    "    \n",
    "    def ask(self, question: str):\n",
    "        out = self.get_answer(question)\n",
    "        return out[\"answer\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference embedding\n",
    "reference_embedding = \"text-embedding-ada-002\"\n",
    "reference_embedding = OpenAIEmbeddings(model=reference_embedding, openai_api_key=openai_api_key)\n",
    "\n",
    "# reference chat\n",
    "reference_chat = \"gpt-4o\"\n",
    "reference_chat = ChatOpenAI(model=reference_chat, openai_api_key=openai_api_key)\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def calc_answer_semantic_similarity(answer, expected):\n",
    "    answer_embedding = np.array(reference_embedding.embed_query(answer))\n",
    "    expected_embedding = np.array(reference_embedding.embed_query(expected))\n",
    "\n",
    "    # https://docs.ragas.io/en/latest/concepts/metrics/semantic_similarity.html\n",
    "    return np.dot(answer_embedding, expected_embedding) / (np.linalg.norm(answer_embedding) * np.linalg.norm(expected_embedding))\n",
    "\n",
    "def calc_answer_relevance(answer, question):\n",
    "    # https://aclanthology.org/2024.eacl-demo.16.pdf\n",
    "    hipot_question = reference_chat.invoke(\n",
    "        f\"Generate a question for the given answer. \\n answer: {answer}\"\n",
    "    ).content\n",
    "    hipot_question_embedding = np.array(reference_embedding.embed_query(hipot_question))\n",
    "    question_embedding = np.array(reference_embedding.embed_query(question))\n",
    "    return np.dot(hipot_question_embedding, question_embedding) / (np.linalg.norm(hipot_question_embedding) * np.linalg.norm(question_embedding))\n",
    "\n",
    "def calc_faithfulness(question, answer):\n",
    "    # https://aclanthology.org/2024.eacl-demo.16.pdf\n",
    "    statements = reference_chat.invoke(\n",
    "        f\"Given a question and answer, create one or more statements from each sentence in the given answer. \\n question: {question} \\n answer: {answer} \\n\\n Do not deviate from the specified format. \\n statement: [statement 1] \\n ... \\n statement: [statement n]\"\n",
    "    ).content\n",
    "    veredicts = reference_chat.invoke(\n",
    "        f\"Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explanation for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. List all veredicts at the end. \\n\\n Statements: \\n {statements}\"\n",
    "    ).content\n",
    "    veredicts = veredicts.split(\"\\n\")[::-1]\n",
    "    veredicts = [veredict.lower() for veredict in veredicts]\n",
    "    n_yes = 0\n",
    "    n_no = 0\n",
    "    for veredict in veredicts:\n",
    "        if \"yes\" in veredict or \"true\" in veredict or \"correct\" in veredict:\n",
    "            n_yes += 1\n",
    "        elif \"no\" in veredict or \"false\" in veredict or \"incorrect\" in veredict:\n",
    "            n_no += 1\n",
    "        else:\n",
    "            break\n",
    "    try:\n",
    "        faithfulness = n_yes / (n_yes + n_no)\n",
    "    except ZeroDivisionError:\n",
    "        faithfulness = 0\n",
    "\n",
    "    return faithfulness\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def eval_model(question: str, expected: str, model_output: dict) -> dict:\n",
    "    question = question\n",
    "    answer = model_output['answer']\n",
    "    context = model_output['context']\n",
    "    expected = expected\n",
    "\n",
    "    answer_semantic_similarity = calc_answer_semantic_similarity(answer, expected)\n",
    "\n",
    "    answer_relevance = calc_answer_relevance(answer, question)\n",
    "\n",
    "    faithfulness = calc_faithfulness(question, answer)\n",
    "\n",
    "    return {\n",
    "        \"end_to_end\": {\n",
    "            \"answer_semantic_similarity\": answer_semantic_similarity, \n",
    "            },\n",
    "        \"component_wise\": {\n",
    "            \"answer_relevance\": answer_relevance,\n",
    "            \"faithfulness\": faithfulness\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: victorsouza.\n",
      "View Weave data at https://wandb.ai/bleu-builders/first-test/weave\n",
      "üç© https://wandb.ai/bleu-builders/first-test/r/call/d7a5d9b9-d432-4259-9260-d978620a9933\n",
      "üç© https://wandb.ai/bleu-builders/first-test/r/call/22bb2160-efe5-4265-8d86-79c1bbfdd870\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'eval_model'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'end_to_end'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'answer_semantic_similarity'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9370337957867892</span><span style=\"font-weight: bold\">}}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'component_wise'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'answer_relevance'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9759252812378427</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'faithfulness'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.72</span><span style=\"font-weight: bold\">}}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.5994775772094725</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'eval_model'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'end_to_end'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'answer_semantic_similarity'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.9370337957867892\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'component_wise'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'answer_relevance'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.9759252812378427\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'faithfulness'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.72\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m4.5994775772094725\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/bleu-builders/first-test/r/call/99cbcd8e-b9a1-4b2e-bc03-01bafef1ef5c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_model': {'end_to_end': {'answer_semantic_similarity': {'mean': 0.9370337957867892}},\n",
       "  'component_wise': {'answer_relevance': {'mean': 0.9759252812378427},\n",
       "   'faithfulness': {'mean': 0.72}}},\n",
       " 'model_latency': {'mean': 4.5994775772094725}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weave.init('first-test')\n",
    "chat_pars = {\n",
    "    \"model\": chat_models[0],\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": None,\n",
    "    \"timeout\": None,\n",
    "    \"max_retries\": 2\n",
    "}\n",
    "\n",
    "prompt_template = f\"\"\"Answer politely the question at the end, using only the following context. The user is not necessarily a specialist, so please avoid jargon and explain any technical terms.\n",
    "\n",
    "<context>\n",
    "{{context}} \n",
    "</context>\n",
    "\n",
    "Question: {{question}}\n",
    "\"\"\"\n",
    "\n",
    "rag = RAGModel(\n",
    "    dbs = dbs,\n",
    "    model_embeddings = embedding_models[0],\n",
    "    chat_pars=chat_pars,\n",
    "    prompt_template = prompt_template\n",
    ")\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    dataset=test_dataset, scorers=[eval_model]\n",
    ")\n",
    "\n",
    "rag.def_faiss_retriever()\n",
    "asyncio.run(evaluation.evaluate(rag.get_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bleu-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
